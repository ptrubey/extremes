{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c3a4b8",
   "metadata": {},
   "source": [
    "**Goal: Variational Inference on parameters of Dirichlet distribution**\n",
    "\n",
    "Model:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i &\\sim \\mathcal{PG}_p(Y\\mid\\alpha_i)\\\\\n",
    "\\alpha_i &\\sim G\\\\\n",
    "G &\\sim \\mathcal{PY}(\\eta, d, G_0)\\\\\n",
    "\\end{aligned}\n",
    "~\\hspace{1cm}~\n",
    "\\begin{aligned}\n",
    "G_0 &= \\prod_{\\ell = 1}^d\\mathcal{G}(\\alpha_{\\ell}\\mid\\xi,\\tau)\\\\\n",
    "\\xi &\\sim \\mathcal{G}(\\xi\\mid a, b)\\\\\n",
    "\\tau &\\sim \\mathcal{G}(\\tau\\mid c,d)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf59a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from numpy.random import gamma\n",
    "\n",
    "from tfprojgamma import ProjectedGamma\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d1dd9",
   "metadata": {},
   "source": [
    "*Declare Random Sample*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db51d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.94761907 0.62279234 0.68411215 0.34912511 4.2482095 ]\n",
      " [0.52249041 3.56568716 0.05637186 2.86972854 0.33627595]\n",
      " [0.99015762 0.45215243 0.33593299 2.89134277 2.43528465]]\n",
      "(0.3, 0.5, 0.2)\n"
     ]
    }
   ],
   "source": [
    "alpha_true = gamma(size = (3,5), shape = 1.5)\n",
    "pi_true = (0.3, 0.5, 0.2)\n",
    "\n",
    "MixProjectedGamma = tfp.distributions.MixtureSameFamily(\n",
    "    mixture_distribution = tfd.Categorical(\n",
    "        probs = pi_true,\n",
    "        ),\n",
    "    components_distribution = ProjectedGamma(\n",
    "        concentration = alpha_true,\n",
    "        )\n",
    "    )\n",
    "print(alpha_true)\n",
    "print(pi_true)\n",
    "Yp = tf.cast(MixProjectedGamma.sample(1000), tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5433e7",
   "metadata": {},
   "source": [
    "*Specifications*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e6856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = Yp.shape; J = 20 # N = nobs, D = ncols, J = nclust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c4bd8",
   "metadata": {},
   "source": [
    "*Prior Parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd123399",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5; b = 0.5    # strength (inherently unstable)\n",
    "c = 2.0; d = 2.0    # rate (biased towards 1)\n",
    "eta, dis = 0.1, 0.1 # PY Strength / Discount Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100c29c",
   "metadata": {},
   "source": [
    "*Define the Joint Distribution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3954925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(N, J, D, eta, discount, dtype = np.float64):\n",
    "    model = tfd.JointDistributionNamed(dict(\n",
    "        xi = tfd.Independent(\n",
    "            tfd.Gamma(\n",
    "                concentration = np.full(D, a, dtype),\n",
    "                rate = np.full(D, b, dtype),\n",
    "                ),\n",
    "            reinterpreted_batch_ndims = 1,\n",
    "            ),\n",
    "        tau = tfd.Independent(\n",
    "            tfd.Gamma(\n",
    "                concentration = np.full(D, c, dtype),\n",
    "                rate = np.full(D, d, dtype),\n",
    "                ),\n",
    "            reinterpreted_batch_ndims = 1,\n",
    "            ),\n",
    "        nu = tfd.Independent(\n",
    "            tfd.Beta(np.ones(K - 1, dtype) - discount, eta + np.arange(1, K) * discount),\n",
    "            reinterpreted_batch_ndims = 1,\n",
    "            ),\n",
    "        alpha = lambda xi, tau: tfd.Independent(\n",
    "            tfd.Gamma(\n",
    "                concentration = np.ones((J, D), dtype) * xi,\n",
    "                rate = np.ones((J, D), dtype) * tau,\n",
    "                ),\n",
    "            reinterpreted_batch_ndims = 2,\n",
    "            ),        \n",
    "        obs = lambda alpha, nu: tfd.Sample(tfd.MixtureSameFamily(\n",
    "            mixture_distribution = tfd.Categorical(probs = stickbreak(nu)),\n",
    "            components_distribution = ProjectedGamma(alpha, np.ones((K, D), dtype)),\n",
    "            sample_shape = (N, D),\n",
    "            )),\n",
    "        ))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc36a9",
   "metadata": {},
   "source": [
    "**Variational Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb562574",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_nu_mu    = tf.Variable(tf.random.normal([J],   dtype = np.float64), name = 'q_nu_mu')\n",
    "q_nu_sd    = tf.Variable(tf.random.normal([J],   dtype = np.tloat64), name = 'q_nu_sd')\n",
    "q_alpha_mu = tf.Variable(tf.random.normal([J,l], dtype = np.float64), name = 'q_alpha_mu')\n",
    "q_alpha_sd = tf.Variable(tf.random.normal([J,l], dtype = np.float64), name = 'q_alpha_sd')\n",
    "q_xi_mu    = tf.Variable(tf.random.normal([l],   dtype = np.float64), name = 'q_xi_mu')\n",
    "q_xi_sd    = tf.Variable(tf.random.normal([l],   dtype = np.float64), name = 'q_xi_sd')\n",
    "q_tau_mu   = tf.Variable(tf.random.normal([l],   dtype = np.float64), name = 'q_tau_mu')\n",
    "q_tau_sd   = tf.variable(tf.random.normal([l],   dtype = np.float64), name = 'q_tau_sd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd408e",
   "metadata": {},
   "source": [
    "**Verifying the structure of the joint model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8caffad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbe8a6",
   "metadata": {},
   "source": [
    "**Mean Field Variational Bayes -- Independence between columns**\n",
    "$$\n",
    "\\log\\alpha \\sim \\prod_{\\ell = 1}^d\\text{Normal}(\\log\\alpha_{\\ell} \\mid \\mu_{q\\ell}, \\sigma_{q\\ell})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfdfa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_mu = tf.Variable(log_alpha_0, dtype = tf.float32)\n",
    "q_scale = tfp.util.TransformedVariable(np.ones(5), tfb.Exp(), dtype = tf.float32)\n",
    "\n",
    "surrogate_posterior = tfd.MultivariateNormalDiag(loc = q_mu, scale_diag = q_scale, name = 'surrogate 1')\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    samples = surrogate_posterior.sample(100)\n",
    "    neg_elbo = -tf.reduce_mean(model_joint_log_prob(samples) - surrogate_posterior.log_prob(samples))\n",
    "print(g.gradient(neg_elbo, surrogate_posterior.trainable_variables)) # exists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20505faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = tfp.vi.fit_surrogate_posterior(\n",
    "    target_log_prob_fn = model_joint_log_prob,\n",
    "    surrogate_posterior = surrogate_posterior,\n",
    "    optimizer = tf.optimizers.Adam(.2),\n",
    "    num_steps = 1000,\n",
    "    sample_size = 500,\n",
    "    )\n",
    "\n",
    "print(tf.exp(q_mu)) # This appears to have worked; the values end in *rougly* the right place.\n",
    "print(q_scale**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240bcd0",
   "metadata": {},
   "source": [
    "**Gaussian Variational Bayes -- Dependence Between Columns**\n",
    "$$\n",
    "\\log\\alpha \\sim \\text{MVNormal}(\\log\\alpha \\mid \\mu_q, \\Sigma_q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6063356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Style: Make the variational Parameters\n",
    "q_nu = tf.Variable(tf.zeros(5, dtype = tf.float32), name = 'Mu Surrogate (mean of log alpha)')\n",
    "cholbijector = tfb.FillScaleTriL(diag_bijector = tfb.Exp())\n",
    "q_Lu = tfp.util.TransformedVariable(tf.eye(5), bijector = cholbijector)\n",
    "\n",
    "surrogate_posterior_mvnorm = tfd.MultivariateNormalTriL(loc = q_nu, scale_tril = q_Lu)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    samples = surrogate_posterior_mvnorm.sample(100)\n",
    "    neg_elbo = -tf.reduce_mean(model_joint_log_prob(samples) - surrogate_posterior_mvnorm.log_prob(samples))\n",
    "print(g.gradient(neg_elbo, surrogate_posterior_mvnorm.trainable_variables)) # Exists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a867e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mvnorm = tfp.vi.fit_surrogate_posterior(\n",
    "    target_log_prob_fn = model_joint_log_prob,\n",
    "    surrogate_posterior = surrogate_posterior_mvnorm,\n",
    "    optimizer = tf.optimizers.Adam(.2),\n",
    "    num_steps = 1000,\n",
    "    sample_size = 500,\n",
    "    )\n",
    "print(tf.exp(q_nu)) # This gives the same basic response as previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf10016",
   "metadata": {},
   "outputs": [],
   "source": [
    "(q_Lu.numpy() @ q_Lu.numpy().T > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1152591",
   "metadata": {},
   "outputs": [],
   "source": [
    "(q_Lu.numpy() @ q_Lu.numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0653197",
   "metadata": {},
   "source": [
    "I guess it makes some sense that the posterior covariance between parameters of the Dirichlet would be positive, despite the covariance between *values* of the Dirichlet being negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
