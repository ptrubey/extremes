{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c3a4b8",
   "metadata": {},
   "source": [
    "**Goal: Variational Inference on BNP Mixture of Projected Gammas**\n",
    "\n",
    "Model:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i\\mid\\alpha_i &\\sim \\text{PG}_p(Y_i\\mid\\alpha_i,1_d)\\\\\n",
    "\\log\\alpha_i &\\sim \\mathcal{G}\\\\\n",
    "G &\\sim \\mathcal{PY}(\\alpha,d,G_0)\\\\\n",
    "\\end{aligned}\n",
    "~\\hspace{1cm}\n",
    "\\begin{aligned}\n",
    "G_0 &= \\mathcal{N}_d\\left(\\log\\alpha\\mid\\mu,\\Sigma\\right)\\\\\n",
    "\\mu &\\sim \\mathcal{N}_d(\\mu_0,I_d)\\\\\n",
    "\\Sigma &\\sim \\mathcal{IW}\\left(\\nu,\\Psi\\right)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf59a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from numpy.random import gamma\n",
    "\n",
    "from tfprojgamma import ProjectedGamma\n",
    "from data import Data\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb03a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stickbreak(v):\n",
    "    \"\"\"\n",
    "    Creates a probability vector (sums to 1) from the vector of independent betas\n",
    "    grabbed from https://luiarthur.github.io/TuringBnpBenchmarks/dpsbgmm \n",
    "        kind of worried about numerical stability here... isn't cumprod going to induce a lot of floating point issues?\n",
    "    \"\"\"\n",
    "    batch_ndims = len(v.shape) - 1\n",
    "    cumprod_one_minus_v = tf.math.cumprod(1 - v, axis=-1)\n",
    "    one_v = tf.pad(v, [[0, 0]] * batch_ndims + [[0, 1]], \"CONSTANT\", constant_values=1)\n",
    "    c_one = tf.pad(cumprod_one_minus_v, [[0, 0]] * batch_ndims + [[1, 0]], \"CONSTANT\", constant_values=1)\n",
    "    return one_v * c_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_model(nDat, nCol, nClust, dtype = np.float32):\n",
    "    return tfd.JointDistributionNamed(dict(\n",
    "        mu = tfd.MultivariateNormalDiag(loc = np.zeros(nCol, dtype), scale_diag = np.ones(nCol, dtype))\n",
    "        Sigma = tfd.TransformedDistribution\n",
    "        \n",
    "        logalpha = tfd.MultivariateNormalTriL(loc = mu, scale_tril = Sigma\n",
    "        \n",
    "        v = tfd.Independent(tfd.Beta(np.ones(nClust - 1, dtype))),\n",
    "    ))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e1faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1032f363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.94761907 0.62279234 0.68411215 0.34912511 4.2482095 ]\n"
     ]
    }
   ],
   "source": [
    "# Set up shape parameters\n",
    "alpha_true = gamma(size = 5, shape = 1.5)\n",
    "print(alpha_true)\n",
    "dist1 = ProjectedGamma(alpha_true, 10)\n",
    "Yp    = tf.cast(dist1.sample(200), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd123399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior shape parameters\n",
    "log_alpha_0 = tf.ones(5, dtype = tf.float32) * np.log(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100c29c",
   "metadata": {},
   "source": [
    "**Define the Joint Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3954925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator\n",
    "def generative_model(log_alpha_0, n_samples):\n",
    "    log_alpha = yield tfd.JointDistributionCoroutine.Root(\n",
    "        tfd.MultivariateNormalDiag(\n",
    "            loc = log_alpha_0, scale_diag = tf.ones(5, dtype = tf.float32), name = 'log_alpha'\n",
    "            ),\n",
    "        )\n",
    "    Yp = yield tfd.Sample(\n",
    "        ProjectedGamma(concentration = tf.exp(log_alpha) * tf.ones((n_samples, 5), dtype = tf.float32)),\n",
    "        name = 'Yp',\n",
    "        )\n",
    "\n",
    "model_joint = tfd.JointDistributionCoroutineAutoBatched(\n",
    "    lambda: generative_model(log_alpha_0, 200),\n",
    "    )\n",
    "\n",
    "model_joint_log_prob = lambda log_alpha: model_joint.log_prob(log_alpha, Yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd408e",
   "metadata": {},
   "source": [
    "**Verifying the structure of the joint model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8caffad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.JointDistributionCoroutineAutoBatched 'JointDistributionCoroutineAutoBatched' batch_shape=[] event_shape=StructTuple(\n",
       "  log_alpha=[5],\n",
       "  Yp=[200, 5]\n",
       ") dtype=StructTuple(\n",
       "  log_alpha=float32,\n",
       "  Yp=float32\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbe8a6",
   "metadata": {},
   "source": [
    "**Mean Field Variational Bayes -- Independence between columns**\n",
    "$$\n",
    "\\log\\alpha \\sim \\prod_{\\ell = 1}^d\\text{Normal}(\\log\\alpha_{\\ell} \\mid \\mu_{q\\ell}, \\sigma_{q\\ell})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebfdfa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([-118.478775,  174.52089 ,  197.55557 ,  481.3123  , -154.02953 ],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([151.6519  , 276.73755 , 280.07654 , 725.90686 ,  67.493996],\n",
      "      dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "q_mu = tf.Variable(log_alpha_0, dtype = tf.float32)\n",
    "q_scale = tfp.util.TransformedVariable(np.ones(5), tfb.Exp(), dtype = tf.float32)\n",
    "\n",
    "surrogate_posterior = tfd.MultivariateNormalDiag(loc = q_mu, scale_diag = q_scale, name = 'surrogate 1')\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    samples = surrogate_posterior.sample(100)\n",
    "    neg_elbo = -tf.reduce_mean(model_joint_log_prob(samples) - surrogate_posterior.log_prob(samples))\n",
    "print(g.gradient(neg_elbo, surrogate_posterior.trainable_variables)) # exists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20505faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3.9468846 0.597943  0.6775192 0.3909855 4.0545163], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.00176153 0.00362453 0.00339159 0.00389452 0.00167895], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "path = tfp.vi.fit_surrogate_posterior(\n",
    "    target_log_prob_fn = model_joint_log_prob,\n",
    "    surrogate_posterior = surrogate_posterior,\n",
    "    optimizer = tf.optimizers.Adam(.2),\n",
    "    num_steps = 1000,\n",
    "    sample_size = 500,\n",
    "    )\n",
    "\n",
    "print(tf.exp(q_mu)) # This appears to have worked; the values end in *rougly* the right place.\n",
    "print(q_scale**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240bcd0",
   "metadata": {},
   "source": [
    "**Gaussian Variational Bayes -- Dependence Between Columns**\n",
    "$$\n",
    "\\log\\alpha \\sim \\text{MVNormal}(\\log\\alpha \\mid \\mu_q, \\Sigma_q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6063356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([-153.65451,  987.5581 ,  499.819  , 1076.0623 , -185.46274],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
      "array([ 208.05101 ,  -68.88415 ,   23.579296,  -62.816746, -142.34943 ,\n",
      "        192.73282 , 1257.155   , -517.5065  ,  -53.48645 ,  140.62529 ,\n",
      "       -450.41083 , 2185.321   ,  676.94464 ,  -16.753181,  -39.66961 ],\n",
      "      dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# New Style: Make the variational Parameters\n",
    "q_nu = tf.Variable(tf.zeros(5, dtype = tf.float32), name = 'Mu Surrogate (mean of log alpha)')\n",
    "cholbijector = tfb.FillScaleTriL(diag_bijector = tfb.Exp())\n",
    "q_Lu = tfp.util.TransformedVariable(tf.eye(5), bijector = cholbijector)\n",
    "\n",
    "surrogate_posterior_mvnorm = tfd.MultivariateNormalTriL(loc = q_nu, scale_tril = q_Lu)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    samples = surrogate_posterior_mvnorm.sample(100)\n",
    "    neg_elbo = -tf.reduce_mean(model_joint_log_prob(samples) - surrogate_posterior_mvnorm.log_prob(samples))\n",
    "print(g.gradient(neg_elbo, surrogate_posterior_mvnorm.trainable_variables)) # Exists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14a867e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3.9388332  0.59376836 0.67372936 0.38574114 4.06507   ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "path_mvnorm = tfp.vi.fit_surrogate_posterior(\n",
    "    target_log_prob_fn = model_joint_log_prob,\n",
    "    surrogate_posterior = surrogate_posterior_mvnorm,\n",
    "    optimizer = tf.optimizers.Adam(.2),\n",
    "    num_steps = 1000,\n",
    "    sample_size = 500,\n",
    "    )\n",
    "print(tf.exp(q_nu)) # This gives the same basic response as previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daf10016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00403725, 0.00055345, 0.00169735, 0.00089325, 0.00248185],\n",
       "       [0.00055345, 0.00411207, 0.00131955, 0.000444  , 0.00097888],\n",
       "       [0.00169735, 0.00131955, 0.0048471 , 0.00037286, 0.00064957],\n",
       "       [0.00089325, 0.000444  , 0.00037286, 0.00465075, 0.00178326],\n",
       "       [0.00248185, 0.00097888, 0.00064957, 0.00178326, 0.00381977]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q_Lu.numpy() @ q_Lu.numpy().T < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0653197",
   "metadata": {},
   "source": [
    "I guess it makes some sense that the posterior covariance between parameters of the Dirichlet would be positive, despite the covariance between *values* of the Dirichlet being negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
