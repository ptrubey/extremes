{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c3a4b8",
   "metadata": {},
   "source": [
    "**Goal: Variational Inference on BNP Mixture of Projected Gammas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from numpy.random import gamma\n",
    "\n",
    "from tfprojgamma import ProjectedGamma\n",
    "from data import Data\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb03a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stickbreak(v):\n",
    "    \"\"\"\n",
    "    Creates a probability vector (sums to 1) from the vector of independent betas\n",
    "    grabbed from https://luiarthur.github.io/TuringBnpBenchmarks/dpsbgmm \n",
    "        kind of worried about numerical stability here... isn't cumprod going to induce a lot of \n",
    "        floating point issues?\n",
    "    \"\"\"\n",
    "    batch_ndims = len(v.shape) - 1\n",
    "    cumprod_one_minus_v = tf.math.cumprod(1 - v, axis=-1)\n",
    "    one_v = tf.pad(v, [[0, 0]] * batch_ndims + [[0, 1]], \"CONSTANT\", constant_values=1)\n",
    "    c_one = tf.pad(cumprod_one_minus_v, [[0, 0]] * batch_ndims + [[1, 0]], \"CONSTANT\", constant_values=1)\n",
    "    return one_v * c_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c1b8f",
   "metadata": {},
   "source": [
    "**Joint Distribution**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i\\mid\\alpha_i &\\sim \\text{PG}_p(Y_i\\mid\\alpha_i,1_d)\\\\\n",
    "\\log\\alpha_i &\\sim \\mathcal{G}\\\\\n",
    "G &\\sim \\mathcal{PY}(\\eta,d,G_0)\\\\\n",
    "\\end{aligned}\n",
    "~\\hspace{1cm}\n",
    "\\begin{aligned}\n",
    "G_0 &= \\mathcal{N}_d\\left(\\log\\alpha\\mid\\mu,\\Sigma\\right)\\\\\n",
    "\\mu &\\sim \\mathcal{N}_d(\\mu_0,I_d)\\\\\n",
    "\\Sigma &\\sim \\mathcal{IW}\\left(\\nu,\\Psi\\right)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Currently building on Dirichlet process with stick-breaking outlined in Ishawaran and James 2001; will incorporate Pitman Yor later.  Where $\\pi_j$ is the prior probability of falling into cluster $j$ and $J$ is the stick-breaking truncation point (maximum number of clusters),\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_j &\\sim \\text{Beta}\\left(1, \\eta\\right) j = 1,\\ldots,J-1\\\\\n",
    "    \\pi_j &= \\begin{cases}\n",
    "        \\prod_{k \\leq j}(1 - \\tau_{k}) \\times \\tau_j &\\text{ for }j = 1,\\ldots,J-1\\\\\n",
    "        \\prod_{k = 1}^{J-1}(1 - \\tau_k) &\\text{ for }j = J\\\\\n",
    "        \\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87e0e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_model(nDat, nCol, nClust, df, conc = 2., rate = 10., dtype = np.float32):\n",
    "    return tfd.JointDistributionNamed(dict(\n",
    "        # Hierarchical Mean\n",
    "        mu = tfd.MultivariateNormalDiag(loc = np.zeros(nCol, dtype), scale_diag = np.ones(nCol, dtype)),\n",
    "        # Hierarchical Covariance\n",
    "        Sigma = tfd.TransformedDistribution(\n",
    "            tfd.WishartTriL(df = df, scale_tril = np.linalg.cholesky(df * np.eye(nCol, dtype = dtype))),\n",
    "            tfb.CholeskyToInvCholesky(),\n",
    "            ),\n",
    "        # DP concentration Parameter\n",
    "        eta = tfd.Gamma(concentration = dtype(conc), rate = dtype(rate)),\n",
    "        # DP Stickbreaking Construction\n",
    "        tau = lambda eta: tfd.Independent(\n",
    "            tfd.Beta(\n",
    "                concentration0 = np.ones(nClust - 1, dtype), \n",
    "                concentration1 = eta[..., tf.newaxis],\n",
    "                )\n",
    "            ),\n",
    "        # log-shape parameters for Projected Gamma\n",
    "        logalpha = lambda mu, Sigma: tfd.Independent(\n",
    "            tfd.MultivariateNormalTriL(loc = mu * np.ones((nClust, 1)), scale_tril = Sigma),\n",
    "            ),\n",
    "        # Likelihood\n",
    "        obs = lambda logalpha, pi: tfd.Sample(\n",
    "            tfd.MixtureSameFamily(\n",
    "                mixture_distribution = tfd.Categorical(probs = stickbreak(tau)),\n",
    "                components_distribution = ProjectedGamma(tf.exp(logalpha)),\n",
    "                ),\n",
    "            sample_shape = nDat,\n",
    "            ),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6302d537",
   "metadata": {},
   "source": [
    "**Import data and pre-process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "758d405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('../datasets/ivt_nov_mar.csv').to_numpy()\n",
    "data = Data(raw, real_vars = list(np.arange(raw.shape[1])), decluster = True)\n",
    "nClust = 100; df = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1338e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model = generative_model(data.nDat, data.nCol, nClust, df = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca561a6",
   "metadata": {},
   "source": [
    "**Gaussian Variational Bayes -- Dependence Between Columns**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu &\\sim \\mathcal{N}\\left(\\mu_{q\\mu}, L_{q\\mu}L_{q\\mu}^T\\right)\\\\\n",
    "L_{\\sigma} &\\sim \\mathcal{IW}\\left(\\nu_{q\\Sigma},\\Psi_{q\\Sigma}\\right)\\\\\n",
    "\\log\\alpha &\\sim \\mathcal{N}_d\n",
    "\n",
    "\\end{aligned}\n",
    "~ \\hspace{1cm}\n",
    "\\begin{aligned} \n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5858639",
   "metadata": {},
   "source": [
    "**Variational Parameters**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu &\\sim \\mathcal{N}_d\\left(\\mu\\mid\\mu_{\\mu},\\Sigma_{\\mu}\\right)\\\\\n",
    "\\log\\alpha_j &\\sim \\mathcal{N}_d\\left(\\mu,\\Sigma)\\\\\n",
    "\\end{aligned}\n",
    "~\\hspace{1cm}~\n",
    "\\begin{aligned}\n",
    "\\tau_j &\\sim \\mathcal{LGN}\\left(\\mu,\\Sigma \\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1156e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "qMuMu   = tf.Variable(tf.random.normal(nCol, dtype = tf.float32), name = 'qMuMu')\n",
    "cholbijector = tfb.FillScaleTriL(diag_bijector = tfb.Exp())\n",
    "qMuLu   = tfp.util.TransformedVariable(tf.eye(nCol), bijector = cholbijector, name = 'qMuLu')\n",
    "qSigmaPsiL = tfp.util.TransformedVariable(tf.eye(nCol), bijector = cholbijector, name = 'qSigmaPsiL')\n",
    "qSigmaNu = tfp.util.TransformedVariable(1., bijector = tfb.Exp(), name = 'qSigmaNu')\n",
    "qLogAlphaMu  = tf.Variable(tf.random.normal((nClust, nCol), name = 'qLogAlpha'))\n",
    "qLogAlphaRho = tf.Variable(tf.random.normal((nClust, nCol), name = 'qLogAlphaRho'))\n",
    "qTauLoc  = tf.Variable(tf.random.normal(nClust - 1), name = 'qTauLoc')\n",
    "qTauRho  = tf.variable(tf.random.normal(nClust - 1), name = 'qTauRho')\n",
    "qEtaLoc = tf.Variable(tf.random.normal([]), name = 'qEtaLoc')\n",
    "qEtaRho = tf.Variable(tf.random.normal([]), name = 'qEtaRho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_posterior = tfd.JointDistributionNamed(dict(\n",
    "    # Hierarchical mean\n",
    "    mu       = tfd.MultivariateNormalTriL(loc = qMuMu, scale_tril = qMuLu),\n",
    "    # Hierarchical Covariance\n",
    "    Sigma    = tfd.TransformedDistribution(\n",
    "        tfd.WishartTriL(df = qSigmaNu, scale_tril = qSigmaPsiL),\n",
    "        tfb.CholeskyToInvCholesky(),\n",
    "        ),\n",
    "    # Cluster Means\n",
    "    logAlpha = tfd.Independent(\n",
    "        tfd.MultivariateNormalTriL(loc = qLogAlphaMu, scale_tril = qLogAlphaRho),\n",
    "        reinterpreted_batch_ndims = 1,\n",
    "        ),\n",
    "    tau = tfd.Independent(\n",
    "        tfd.LogitNormal(qTauLoc, tf.nn.softplus(qTauRho)), \n",
    "        reinterpreted_batch_ndims = 1,\n",
    "        ),\n",
    "    eta = tfd.LogNormal(qEtaLoc, tf.nn.softplus(qEtaRho)),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240bcd0",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\log\\alpha \\sim \\text{MVNormal}(\\log\\alpha \\mid \\mu_q, \\Sigma_q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6063356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Style: Make the variational Parameters\n",
    "q_nu = tf.Variable(tf.zeros(5, dtype = tf.float32), name = 'Mu Surrogate (mean of log alpha)')\n",
    "cholbijector = tfb.FillScaleTriL(diag_bijector = tfb.Exp())\n",
    "q_Lu = tfp.util.TransformedVariable(tf.eye(5), bijector = cholbijector)\n",
    "\n",
    "surrogate_posterior_mvnorm = tfd.MultivariateNormalTriL(loc = q_nu, scale_tril = q_Lu)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    samples = surrogate_posterior_mvnorm.sample(100)\n",
    "    neg_elbo = -tf.reduce_mean(model_joint_log_prob(samples) - surrogate_posterior_mvnorm.log_prob(samples))\n",
    "print(g.gradient(neg_elbo, surrogate_posterior_mvnorm.trainable_variables)) # Exists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a867e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mvnorm = tfp.vi.fit_surrogate_posterior(\n",
    "    target_log_prob_fn = model_joint_log_prob,\n",
    "    surrogate_posterior = surrogate_posterior_mvnorm,\n",
    "    optimizer = tf.optimizers.Adam(.2),\n",
    "    num_steps = 1000,\n",
    "    sample_size = 500,\n",
    "    )\n",
    "print(tf.exp(q_nu)) # This gives the same basic response as previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf10016",
   "metadata": {},
   "outputs": [],
   "source": [
    "(q_Lu.numpy() @ q_Lu.numpy().T < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0653197",
   "metadata": {},
   "source": [
    "I guess it makes some sense that the posterior covariance between parameters of the Dirichlet would be positive, despite the covariance between *values* of the Dirichlet being negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
