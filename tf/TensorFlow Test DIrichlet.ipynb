{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c3a4b8",
   "metadata": {},
   "source": [
    "**Goal: Variational Inference on parameters of Dirichlet distribution**\n",
    "\n",
    "Model:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i &\\sim \\text{Dirichlet}(Y\\mid\\alpha)\\\\\n",
    "\\log\\alpha &\\sim \\text{MVNormal}(\\log\\alpha\\mid \\log(0.5), I_d)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf59a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from numpy.random import gamma\n",
    "\n",
    "from tfprojgamma import ProjectedGamma\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1032f363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.94761907 0.62279234 0.68411215 0.34912511 4.2482095 ]\n"
     ]
    }
   ],
   "source": [
    "# Set up shape parameters\n",
    "alpha_true = gamma(size = 5, shape = 1.5)\n",
    "print(alpha_true)\n",
    "# dist1 = ProjectedGamma(alpha_true, 10)\n",
    "# Yp    = dist1.sample(200)\n",
    "dist1 = tfd.Dirichlet(concentration = alpha_true)\n",
    "Y = tf.cast(dist1.sample(200), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd123399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior shape parameters\n",
    "log_alpha_0 = tf.ones(5, dtype = tf.float32) * np.log(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100c29c",
   "metadata": {},
   "source": [
    "**Define the Joint Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3954925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator\n",
    "def generative_model(log_alpha_0, n_samples):\n",
    "    log_alpha = yield tfd.JointDistributionCoroutine.Root(\n",
    "        tfd.MultivariateNormalDiag(\n",
    "            loc = log_alpha_0, scale_diag = tf.ones(5, dtype = tf.float32), name = 'log_alpha'\n",
    "            ),\n",
    "        )\n",
    "    Yp = yield tfd.Sample(\n",
    "        tfd.Dirichlet(concentration = tf.exp(log_alpha) * tf.ones((n_samples, 5), dtype = tf.float32)),\n",
    "        name = 'Yp',\n",
    "        )\n",
    "\n",
    "model_joint = tfd.JointDistributionCoroutineAutoBatched(\n",
    "    lambda: generative_model(log_alpha_0, 200),\n",
    "    )\n",
    "\n",
    "model_joint_log_prob = lambda log_alpha: model_joint.log_prob(log_alpha, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd408e",
   "metadata": {},
   "source": [
    "**Verifying the structure of the joint model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8caffad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.JointDistributionCoroutineAutoBatched 'JointDistributionCoroutineAutoBatched' batch_shape=[] event_shape=StructTuple(\n",
       "  log_alpha=[5],\n",
       "  Yp=[200, 5]\n",
       ") dtype=StructTuple(\n",
       "  log_alpha=float32,\n",
       "  Yp=float32\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbe8a6",
   "metadata": {},
   "source": [
    "**Mean Field Variational Bayes -- Independence between columns**\n",
    "$$\n",
    "\\log\\alpha \\sim \\prod_{\\ell = 1}^d\\text{Normal}(\\log\\alpha_{\\ell} \\mid \\mu_{q\\ell}, \\sigma_{q\\ell})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebfdfa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([-161.76694,  399.62607,  284.2843 ,  511.3727 , -201.02748],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([ 85.67609, 698.04956, 525.29767, 713.74994,  65.32199],\n",
      "      dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "q_mu = tf.Variable(log_alpha_0, dtype = tf.float32)\n",
    "q_scale = tfp.util.TransformedVariable(np.ones(5), tfb.Exp(), dtype = tf.float32)\n",
    "\n",
    "surrogate_posterior = tfd.MultivariateNormalDiag(loc = q_mu, scale_diag = q_scale, name = 'surrogate 1')\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    samples = surrogate_posterior.sample(100)\n",
    "    neg_elbo = -tf.reduce_mean(model_joint_log_prob(samples) - surrogate_posterior.log_prob(samples))\n",
    "print(g.gradient(neg_elbo, surrogate_posterior.trainable_variables)) # exists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20505faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.446111   0.6254502  0.75169945 0.4220706  4.878486  ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "path = tfp.vi.fit_surrogate_posterior(\n",
    "    target_log_prob_fn = model_joint_log_prob,\n",
    "    surrogate_posterior = surrogate_posterior,\n",
    "    optimizer = tf.optimizers.Adam(.2),\n",
    "    num_steps = 1000,\n",
    "    sample_size = 500,\n",
    "    )\n",
    "\n",
    "print(tf.exp(q_mu)) # This appears to have worked; the values end in *rougly* the right place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240bcd0",
   "metadata": {},
   "source": [
    "**Gaussian Variational Bayes -- Dependence Between Columns**\n",
    "$$\n",
    "\\log\\alpha \\sim \\text{MVNormal}(\\log\\alpha \\mid \\mu_q, \\Sigma_q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6063356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([-162.54315,  855.37195,  544.6034 , 1385.1965 , -174.64993],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
      "array([ 308.8647  , -122.44166 ,   27.030043,  -40.740246, -138.23465 ,\n",
      "        177.29811 , 2592.4873  , -977.86127 , -904.6997  ,  440.0187  ,\n",
      "        170.37288 , 1404.0281  , 1037.0505  ,   64.93281 ,  265.7789  ],\n",
      "      dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# New Style: Make the variational Parameters\n",
    "q_nu = tf.Variable(tf.zeros(5, dtype = tf.float32), name = 'Mu Surrogate (mean of log alpha)')\n",
    "cholbijector = tfb.FillScaleTriL(diag_bijector = tfb.Exp())\n",
    "q_Lu = tfp.util.TransformedVariable(tf.eye(5), bijector = cholbijector)\n",
    "\n",
    "surrogate_posterior_mvnorm = tfd.MultivariateNormalTriL(loc = q_nu, scale_tril = q_Lu)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    samples = surrogate_posterior_mvnorm.sample(100)\n",
    "    neg_elbo = -tf.reduce_mean(model_joint_log_prob(samples) - surrogate_posterior_mvnorm.log_prob(samples))\n",
    "print(g.gradient(neg_elbo, surrogate_posterior_mvnorm.trainable_variables)) # Exists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a867e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.4074626 0.6281958 0.7440691 0.4203112 4.9172263], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "path_mvnorm = tfp.vi.fit_surrogate_posterior(\n",
    "    target_log_prob_fn = model_joint_log_prob,\n",
    "    surrogate_posterior = surrogate_posterior_mvnorm,\n",
    "    optimizer = tf.optimizers.Adam(.2),\n",
    "    num_steps = 1000,\n",
    "    sample_size = 500,\n",
    "    )\n",
    "print(tf.exp(q_nu)) # This gives the same basic response as previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf10016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q_Lu.numpy() @ q_Lu.numpy().T) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0653197",
   "metadata": {},
   "source": [
    "I guess it makes some sense that the posterior covariance between parameters of the Dirichlet would be positive, despite the covariance between *values* of the Dirichlet being negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
